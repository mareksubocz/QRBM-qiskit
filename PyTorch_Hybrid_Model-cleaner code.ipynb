{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import IBMQ\n",
    "# IBMQ.save_account(MY_API_TOKEN)\n",
    "import qiskit\n",
    "qiskit.__version__\n",
    "\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister\n",
    "from qiskit.quantum_info.operators import Operator\n",
    "from qiskit import(QuantumCircuit, execute, Aer)\n",
    "from qiskit.visualization import plot_histogram\n",
    "from qiskit.extensions import Initialize # Import the Inititialize function\n",
    "from qiskit.aqua.circuits.gates import multi_control_toffoli_gate\n",
    "from qiskit.aqua.circuits.gates import multi_control_multi_target_gate\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline   \n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage import img_as_bool\n",
    "\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exctract_single_qubit_measurment(dict_of_counts, qubit_range):\n",
    "#     print(dict_of_counts)\n",
    "#     print(len(list(dict_of_counts.keys())[0]))\n",
    "    num_qubits = len(list(dict_of_counts.keys())[0])\n",
    "#     result = np.zeros(len(qubit_range))\n",
    "    result = np.zeros(num_qubits)\n",
    "#     print(result)\n",
    "    for el in dict_of_counts:\n",
    "        for i in range(num_qubits):\n",
    "#             print(\"i\", i)\n",
    "#             print(\"el[i]\", el[i])\n",
    "            if i in qubit_range and el[i] == '1':\n",
    "                result[i] += dict_of_counts[el]\n",
    "#     print(result)\n",
    "#     print(result[qubit_range])\n",
    "    return result[qubit_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytywanie danych inną metodą\n",
    "# Concentrating on the first 100 samples\n",
    "n_samples = 100\n",
    "\n",
    "X_train = datasets.MNIST(root='./data', train=True, download=True,\n",
    "                         transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# Leaving only labels 0 and 1 \n",
    "idx = np.append(np.where(X_train.targets == 0)[0][:n_samples], \n",
    "                np.where(X_train.targets == 1)[0][:n_samples])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = X_train.targets[idx]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples_show = 6\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "while n_samples_show > 0:\n",
    "    images, targets = data_iter.__next__()\n",
    "\n",
    "    axes[n_samples_show - 1].imshow(images[0].numpy().squeeze(), cmap='gray')\n",
    "    axes[n_samples_show - 1].set_xticks([])\n",
    "    axes[n_samples_show - 1].set_yticks([])\n",
    "    axes[n_samples_show - 1].set_title(\"Labeled: {}\".format(targets.item()))\n",
    "    \n",
    "    n_samples_show -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "\n",
    "X_test = datasets.MNIST(root='./data', train=False, download=True,\n",
    "                        transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "idx = np.append(np.where(X_test.targets == 0)[0][:n_samples], \n",
    "                np.where(X_test.targets == 1)[0][:n_samples])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = X_test.targets[idx]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMLCircuit():\n",
    "    def __init__(self, visible, hidden, num_shots=1000):\n",
    "        self.visible = visible\n",
    "        self.hidden = hidden\n",
    "        self.ancilla = visible-1\n",
    "        self.qr = QuantumRegister((self.visible + self.hidden + self.ancilla), 'q')\n",
    "        self.cr = ClassicalRegister(self.hidden, 'c')\n",
    "        self.qc = QuantumCircuit(self.qr, self.cr)\n",
    "\n",
    "        self.num_shots = num_shots\n",
    "    \n",
    "#     def run(self, thetas):\n",
    "    def circuit_function(self, x, weight_matrix):\n",
    "        self.qc.data = []\n",
    "#         print(\"x: \",x)\n",
    "#         print(\"x[0]: \", x[0])\n",
    "\n",
    "        # inicjalizacja wartości qubitów wejściowych (x)\n",
    "        initial_state = [[np.sqrt(1-x[i]*x[i]), x[i]] for i in range(len(x))]\n",
    "\n",
    "        # inicjalizacja wartości qubitów wejściowych i bramka Hadamarda\n",
    "        for i in range(visible):\n",
    "            initialize_qubit = Initialize(initial_state[i])\n",
    "            self.qc.append(initialize_qubit, [i])\n",
    "            self.qc.h(i)\n",
    "\n",
    "        # ciąg bramek CNOT i bramek rotacji R (zależnych od parametrów)\n",
    "        for i in range(self.hidden):\n",
    "            for j in range(self.visible):\n",
    "                self.qc.ry(weight_matrix[j][i], j)\n",
    "#             print([self.qr[k] for k in range(self.visible)])\n",
    "#             print(self.qr[self.visible + i])\n",
    "#             print([self.qr[i] for i in range(self.visible + self.hidden, self.visible + self.hidden + self.ancilla)])\n",
    "            multi_control_toffoli_gate.mct(self.qc, [self.qr[k] for k in range(self.visible)], self.qr[self.visible + i], [self.qr[i] for i in range(self.visible + self.hidden, self.visible + self.hidden + self.ancilla)], mode='basic')\n",
    "\n",
    "        # pomiar linii visible\n",
    "        self.qc.measure(list(range(self.visible, self.visible+self.hidden)), list(range(self.hidden)))\n",
    "\n",
    "        #eksperyment:\n",
    "        simulator = Aer.get_backend('qasm_simulator')\n",
    "        job = execute(self.qc, simulator, shots=self.num_shots)\n",
    "        result = job.result()\n",
    "        counts = result.get_counts(self.qc)\n",
    "        ph = exctract_single_qubit_measurment(counts, list(range(self.hidden))) / self.num_shots\n",
    "    #     print(\"\\nProbabilities are:\",ph)\n",
    "        return ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ph:  [0.003 0.03 ]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌─────────────────────────────┐┌───┐ ┌────────────┐      ┌────────────┐»\n",
       "q_0: |0>┤ Initialize(0.87976,0.47542) ├┤ H ├─┤ Ry(2.7776) ├──■───┤ Ry(2.5897) ├»\n",
       "        ├─────────────────────────────┤├───┤┌┴────────────┤  │  ┌┴────────────┤»\n",
       "q_1: |0>┤ Initialize(0.52219,0.85283) ├┤ H ├┤ Ry(0.14324) ├──■──┤ Ry(0.54825) ├»\n",
       "        └─────────────────────────────┘└───┘└─────────────┘┌─┴─┐└─────┬─┬─────┘»\n",
       "q_2: |0>───────────────────────────────────────────────────┤ X ├──────┤M├──────»\n",
       "                                                           └───┘      └╥┘      »\n",
       "q_3: |0>───────────────────────────────────────────────────────────────╫───────»\n",
       "                                                                       ║       »\n",
       "q_4: |0>───────────────────────────────────────────────────────────────╫───────»\n",
       "                                                                       ║       »\n",
       " c_0: 0 ═══════════════════════════════════════════════════════════════╩═══════»\n",
       "                                                                               »\n",
       " c_1: 0 ═══════════════════════════════════════════════════════════════════════»\n",
       "                                                                               »\n",
       "«             \n",
       "«q_0: ──■─────\n",
       "«       │     \n",
       "«q_1: ──■─────\n",
       "«       │     \n",
       "«q_2: ──┼─────\n",
       "«     ┌─┴─┐┌─┐\n",
       "«q_3: ┤ X ├┤M├\n",
       "«     └───┘└╥┘\n",
       "«q_4: ──────╫─\n",
       "«           ║ \n",
       "«c_0: ══════╬═\n",
       "«           ║ \n",
       "«c_1: ══════╩═\n",
       "«             </pre>"
      ],
      "text/plain": [
       "        ┌─────────────────────────────┐┌───┐ ┌────────────┐      ┌────────────┐»\n",
       "q_0: |0>┤ Initialize(0.87976,0.47542) ├┤ H ├─┤ Ry(2.7776) ├──■───┤ Ry(2.5897) ├»\n",
       "        ├─────────────────────────────┤├───┤┌┴────────────┤  │  ┌┴────────────┤»\n",
       "q_1: |0>┤ Initialize(0.52219,0.85283) ├┤ H ├┤ Ry(0.14324) ├──■──┤ Ry(0.54825) ├»\n",
       "        └─────────────────────────────┘└───┘└─────────────┘┌─┴─┐└─────┬─┬─────┘»\n",
       "q_2: |0>───────────────────────────────────────────────────┤ X ├──────┤M├──────»\n",
       "                                                           └───┘      └╥┘      »\n",
       "q_3: |0>───────────────────────────────────────────────────────────────╫───────»\n",
       "                                                                       ║       »\n",
       "q_4: |0>───────────────────────────────────────────────────────────────╫───────»\n",
       "                                                                       ║       »\n",
       " c_0: 0 ═══════════════════════════════════════════════════════════════╩═══════»\n",
       "                                                                               »\n",
       " c_1: 0 ═══════════════════════════════════════════════════════════════════════»\n",
       "                                                                               »\n",
       "«             \n",
       "«q_0: ──■─────\n",
       "«       │     \n",
       "«q_1: ──■─────\n",
       "«       │     \n",
       "«q_2: ──┼─────\n",
       "«     ┌─┴─┐┌─┐\n",
       "«q_3: ┤ X ├┤M├\n",
       "«     └───┘└╥┘\n",
       "«q_4: ──────╫─\n",
       "«           ║ \n",
       "«c_0: ══════╬═\n",
       "«           ║ \n",
       "«c_1: ══════╩═\n",
       "«             "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visible = 2\n",
    "hidden = 2\n",
    "    \n",
    "QMLC = QMLCircuit(visible, hidden, 1000)\n",
    "#definicja wejścia (x)oraz inicjalizacja macierzy wag\n",
    "x = np.array([random.uniform(0, 1) for n in range(visible)])\n",
    "weight_matrix = np.random.rand(visible, hidden) * np.pi\n",
    "ph = QMLC.circuit_function(x, weight_matrix)\n",
    "print(\"ph: \", ph)\n",
    "QMLC.qc.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global weight_matrix\n",
    "# weight_matrix = nn.Parameter(torch.tensor(np.random.rand(visible, hidden) * np.pi))\n",
    "# print(\"self.weight_matrix: \", weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(Function):\n",
    "    \"\"\" Hybrid quantum - classical function definition \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight_matrix, QMLC, epsilon):\n",
    "        \"\"\" Forward pass computation \"\"\"\n",
    "        print(\"FORWARD BEGIN\")\n",
    "        ctx.epsilon = epsilon\n",
    "        ctx.QMLC = QMLC\n",
    "        ctx.weight_matrix = weight_matrix\n",
    "        print(\"input from forward: \", input)\n",
    "        \n",
    "        print(\"weight_matrix: \", ctx.weight_matrix)\n",
    "        \n",
    "        wm = ctx.weight_matrix.tolist()\n",
    "        print(\"wm: \", wm)\n",
    "        \n",
    "        ph = ctx.QMLC.circuit_function(input.tolist()[0], wm)\n",
    "        \n",
    "        \n",
    "        result = torch.tensor([ph])\n",
    "        print(\"result: \", result)\n",
    "        ctx.save_for_backward(input, result)\n",
    "        print(\"FORWARD END\")\n",
    "        return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        print(\"BACKWARD BEGIN\")\n",
    "        print(\"grad_output: \", grad_output)\n",
    "        input, ph = ctx.saved_tensors\n",
    "        input_list = np.array(input.tolist())\n",
    "        wm = ctx.weight_matrix.tolist()\n",
    "\n",
    "#         print(\"wm: \", wm)\n",
    "#         print(len(wm))\n",
    "#         print(len(wm[0]))\n",
    "        gradient = []\n",
    "        # wyliczam część gradientu dy/dw\n",
    "        for i in range(len(wm)):\n",
    "            gradient_row = []\n",
    "            for j in range(len(wm[0])):\n",
    "                wm[i][j] += ctx.epsilon\n",
    "                result_plus = ctx.QMLC.circuit_function(input_list.tolist()[0], wm)\n",
    "                print(wm)\n",
    "\n",
    "                wm[i][j] -= 2*ctx.epsilon\n",
    "                result_minus = ctx.QMLC.circuit_function(input_list.tolist()[0], wm)\n",
    "                print(wm)\n",
    "\n",
    "                wm[i][j] += ctx.epsilon\n",
    "                result_0 = ctx.QMLC.circuit_function(input_list.tolist()[0], wm)\n",
    "                print(wm)\n",
    "    #             print(\"exp_ph\", expected_ph)\n",
    "    #             print(result_plus - result_minus)\n",
    "                dydw = (result_plus - result_minus)/(2*ctx.epsilon)\n",
    "                djdy = (result_0 - grad_output.tolist())\n",
    "                lr = 0.05\n",
    "                print(\"dydw: \", dydw)\n",
    "                print(\"djdy: \", djdy)\n",
    "                result = djdy * dydw * lr\n",
    "#                 print(\"result: \", result)\n",
    "                gradient_row.append(np.sum(result))\n",
    "            gradient.append(gradient_row)\n",
    "#         print(\"gradient: \", gradient)\n",
    "        gradient = np.array(gradient)\n",
    "#         print(\"gradient size: \", gradient)\n",
    "#         print(\"wm size: \", wm)\n",
    "        wm -= gradient\n",
    "#         print(\"self.weight_matrix: \", wm)\n",
    "#         print(torch.tensor(gradient).float() * grad_output.float(), None, None)\n",
    "#         print(\"BACKWARD END\")\n",
    "        ret = np.zeros((len(wm), 1)).T\n",
    "#         print(ret)\n",
    "        return torch.tensor(ret).float(), None, None, None\n",
    "\n",
    "\n",
    "class QuantumLayer(nn.Module):\n",
    "    \"\"\" Hybrid quantum - classical layer definition \"\"\"\n",
    "    \n",
    "    def __init__(self, visible, hidden, backend, shots, epsilon):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        self.visible = visible\n",
    "        self.hidden = hidden\n",
    "        self.QMLC = QMLCircuit(self.visible, self.hidden, 1000)\n",
    "        self.epsilon = epsilon\n",
    "#         self.weight_matrix = nn.Parameter(torch.randn(visible, hidden))\n",
    "#         self.weight_matrix = np.random.rand(visible, hidden) * np.pi\n",
    "        self.weight_matrix = nn.Parameter(torch.tensor(np.random.rand(visible, hidden) * np.pi))\n",
    "        print(\"self.weight_matrix: \", self.weight_matrix)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return QFunction.apply(input, self.weight_matrix, self.QMLC, self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(6400, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        self.quantum = QuantumLayer(2, 2, qiskit.Aer.get_backend('qasm_simulator'), 1000, 0.01)\n",
    "        self.fc3 = nn.Linear(2, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 6400)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x)\n",
    "#         print(\"softmax x: \", x)\n",
    "        x = self.quantum(x)\n",
    "#         print(\"x after quantum: \", x)\n",
    "#         print(\"x after quantum: \", x.size())\n",
    "        x = self.fc3(x.float())\n",
    "#         print(\"x after fc3: \", x.size())\n",
    "#         print(\"x after fc3: \", x)\n",
    "        return F.log_softmax(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "epoch:  0\n",
      "weight_matrix:  [[2.77764441 2.58965696]\n",
      " [0.14324223 0.54824866]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5403, 0.4597]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4830, 0.4500]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5139,  0.2661]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mateusz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "c:\\users\\mateusz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dydw:  [-0.8 -0.6]\n",
      "djdy:  [[1.01885534 0.21290162]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.6 -1.2]\n",
      "djdy:  [[1.01185534 0.20290162]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [1.   1.05]\n",
      "djdy:  [[1.02585534 0.19690162]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.65 -1.3 ]\n",
      "djdy:  [[1.00785534 0.21090162]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5405, 0.4595]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4880, 0.4700]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5155,  0.2657]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.45 -1.05]\n",
      "djdy:  [[1.03149011 0.20632669]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.2 -2.3]\n",
      "djdy:  [[0.99349011 0.17732669]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.8   0.45]\n",
      "djdy:  [[1.00149011 0.17432669]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-1.05  0.2 ]\n",
      "djdy:  [[1.02949011 0.18932669]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5329, 0.4671]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5340, 0.4650]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.4973,  0.2551]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.   -0.85]\n",
      "djdy:  [[1.00629735 0.21192654]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.05  1.  ]\n",
      "djdy:  [[1.00729735 0.20492654]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.2  0.7]\n",
      "djdy:  [[0.96629735 0.21292654]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-3.4  1.2]\n",
      "djdy:  [[0.98529735 0.22092654]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5283, 0.4717]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5070, 0.4430]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[ 0.7319, -0.3736]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.2  0.3]\n",
      "djdy:  [[-0.20988031  0.8036012 ]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [1.1 0.7]\n",
      "djdy:  [[-0.22788031  0.8366012 ]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.9  -0.65]\n",
      "djdy:  [[-0.21388031  0.8256012 ]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.45 -0.85]\n",
      "djdy:  [[-0.23788031  0.8346012 ]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5397, 0.4603]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5170, 0.4670]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[ 0.7317, -0.3730]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.55 -0.35]\n",
      "djdy:  [[-0.24266639  0.84100617]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 1.55 -1.  ]\n",
      "djdy:  [[-0.24666639  0.86200617]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 1.35 -0.55]\n",
      "djdy:  [[-0.25566639  0.84200617]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.2  -1.35]\n",
      "djdy:  [[-0.24366639  0.85500617]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5467, 0.4533]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5090, 0.4460]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5025,  0.2563]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.75 -0.05]\n",
      "djdy:  [[1.00647121 0.22270676]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.25 -0.9 ]\n",
      "djdy:  [[0.99547121 0.21670676]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 1.4 -0.5]\n",
      "djdy:  [[0.99647121 0.21470676]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [ 1.  -0.1]\n",
      "djdy:  [[1.00747121 0.20270676]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5454, 0.4546]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4860, 0.4970]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5206,  0.2654]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 2.35 -0.25]\n",
      "djdy:  [[1.02656831 0.17559271]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.7 -0.6]\n",
      "djdy:  [[0.99956831 0.20359271]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [1.15 1.6 ]\n",
      "djdy:  [[1.01056831 0.19059271]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.3  -0.15]\n",
      "djdy:  [[1.06656831 0.20559271]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5396, 0.4604]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5220, 0.4530]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.4989,  0.2540]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [0.3  0.25]\n",
      "djdy:  [[1.0058665  0.21696524]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [0.65 0.2 ]\n",
      "djdy:  [[0.9988665  0.17796524]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.1  -0.05]\n",
      "djdy:  [[1.0158665  0.20796524]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [1.  0.7]\n",
      "djdy:  [[1.0478665  0.17796524]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5453, 0.4547]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4840, 0.4910]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[ 0.7163, -0.3641]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.4   0.15]\n",
      "djdy:  [[-0.20927414  0.84610624]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 1.55 -0.9 ]\n",
      "djdy:  [[-0.20627414  0.83510624]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [0.9  0.35]\n",
      "djdy:  [[-0.20827414  0.83710624]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.95  0.1 ]\n",
      "djdy:  [[-0.22827414  0.85810624]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5426, 0.4574]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4850, 0.4840]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[ 0.7182, -0.3649]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.25 -0.7 ]\n",
      "djdy:  [[-0.21819878  0.83692318]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [1.9 1.6]\n",
      "djdy:  [[-0.20919878  0.80692318]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 1.7 -1.4]\n",
      "djdy:  [[-0.20719878  0.84692318]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [0.45 1.5 ]\n",
      "djdy:  [[-0.23719878  0.82892318]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5374, 0.4626]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4750, 0.4860]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[ 0.7139, -0.3629]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.  -1.8]\n",
      "djdy:  [[-0.19186284  0.8119086 ]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.65  0.8 ]\n",
      "djdy:  [[-0.21886284  0.8289086 ]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.2  1.8]\n",
      "djdy:  [[-0.20186284  0.8089086 ]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [1.75 0.45]\n",
      "djdy:  [[-0.21586284  0.8369086 ]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5458, 0.4542]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5200, 0.4740]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[ 0.7322, -0.3727]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.35 -1.35]\n",
      "djdy:  [[-0.25023025  0.82771225]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 2.1 -1.3]\n",
      "djdy:  [[-0.24023025  0.84571225]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.65 -0.55]\n",
      "djdy:  [[-0.21323025  0.84671225]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-1.7  1.4]\n",
      "djdy:  [[-0.21923025  0.83171225]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5302, 0.4698]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5030, 0.4650]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[ 0.7268, -0.3706]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.95 -2.1 ]\n",
      "djdy:  [[-0.23075925  0.82461295]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.1  -1.45]\n",
      "djdy:  [[-0.23175925  0.84961295]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [2.25 1.4 ]\n",
      "djdy:  [[-0.19175925  0.82461295]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-1.35  0.75]\n",
      "djdy:  [[-0.20175925  0.83761295]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5427, 0.4573]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5160, 0.4610]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5029,  0.2570]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.8 -0.4]\n",
      "djdy:  [[0.9908832  0.22595923]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-3.05 -0.3 ]\n",
      "djdy:  [[1.0058832  0.21995923]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [1.1 0.2]\n",
      "djdy:  [[0.9928832  0.21795923]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-1.35  2.45]\n",
      "djdy:  [[0.9808832  0.23895923]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5384, 0.4616]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4880, 0.4860]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5181,  0.2652]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.35  0.5 ]\n",
      "djdy:  [[1.02606265 0.19479187]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.65 -0.5 ]\n",
      "djdy:  [[1.03106265 0.18079187]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.6  2.3]\n",
      "djdy:  [[1.01206265 0.20079187]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.75 -0.45]\n",
      "djdy:  [[1.01506265 0.19679187]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5284, 0.4716]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5140, 0.4420]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5002,  0.2563]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 1.4 -0.3]\n",
      "djdy:  [[1.00523544 0.20370056]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.8  0.1]\n",
      "djdy:  [[0.99423544 0.18270056]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-2.1   0.45]\n",
      "djdy:  [[1.00323544 0.17870056]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [ 1.55 -1.3 ]\n",
      "djdy:  [[1.02923544 0.17970056]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5373, 0.4627]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4960, 0.4580]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5099,  0.2613]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [0.15 1.  ]\n",
      "djdy:  [[1.03889401 0.21167682]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 2.4 -0.5]\n",
      "djdy:  [[1.02089401 0.18967682]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.9   0.45]\n",
      "djdy:  [[1.02689401 0.16967682]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-1.05 -0.5 ]\n",
      "djdy:  [[1.01789401 0.19267682]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5388, 0.4612]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5110, 0.4620]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[ 0.7280, -0.3730]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.4  -0.65]\n",
      "djdy:  [[-0.2220165   0.84803033]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.45 -1.15]\n",
      "djdy:  [[-0.1840165   0.82303033]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.45  1.75]\n",
      "djdy:  [[-0.2090165   0.84503033]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.75  1.95]\n",
      "djdy:  [[-0.2180165   0.82903033]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5443, 0.4557]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5160, 0.4580]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5026,  0.2576]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.95  1.8 ]\n",
      "djdy:  [[1.00956056 0.18536912]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.75 -0.9 ]\n",
      "djdy:  [[1.02356056 0.20836912]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [0.45 0.1 ]\n",
      "djdy:  [[1.02056056 0.20736912]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-1.05  0.1 ]\n",
      "djdy:  [[1.02356056 0.20636912]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5363, 0.4637]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4980, 0.4650]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[ 0.7225, -0.3704]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.55  1.5 ]\n",
      "djdy:  [[-0.2104738   0.82735504]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 1.25 -2.8 ]\n",
      "djdy:  [[-0.2354738   0.84035504]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.5   0.55]\n",
      "djdy:  [[-0.2114738   0.84035504]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.35  0.35]\n",
      "djdy:  [[-0.1814738   0.81935504]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5416, 0.4584]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5190, 0.4470]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.4994,  0.2562]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.8  -0.15]\n",
      "djdy:  [[1.00144243 0.21281939]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.1  -0.75]\n",
      "djdy:  [[1.04544243 0.21481939]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.85 -0.05]\n",
      "djdy:  [[1.00044243 0.21181939]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [ 2.95 -0.5 ]\n",
      "djdy:  [[1.02644243 0.19281939]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5506, 0.4494]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4980, 0.4740]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5123,  0.2628]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.35  0.25]\n",
      "djdy:  [[1.01526974 0.20721012]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 1.15 -0.5 ]\n",
      "djdy:  [[1.01026974 0.21521012]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.5  1.6]\n",
      "djdy:  [[1.02126974 0.19621012]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.2   1.65]\n",
      "djdy:  [[0.99126974 0.21521012]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5409, 0.4591]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.4910, 0.4890]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5177,  0.2655]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.6  -0.05]\n",
      "djdy:  [[1.02668017 0.20352792]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.05 -0.85]\n",
      "djdy:  [[1.04768017 0.16152792]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.85  0.4 ]\n",
      "djdy:  [[1.03468017 0.22152792]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.3 -1.4]\n",
      "djdy:  [[0.99168017 0.19352792]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5399, 0.4601]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5140, 0.4480]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5014,  0.2569]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.85  0.35]\n",
      "djdy:  [[1.00139922 0.2020765 ]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.85 -0.6 ]\n",
      "djdy:  [[1.00639922 0.2060765 ]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.25 -0.4 ]\n",
      "djdy:  [[1.00539922 0.2060765 ]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [ 1.15 -0.1 ]\n",
      "djdy:  [[1.02439922 0.2180765 ]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5410, 0.4590]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5190, 0.4390]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.4978,  0.2548]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 2.05 -1.  ]\n",
      "djdy:  [[1.04479603 0.19120957]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.75 -0.1 ]\n",
      "djdy:  [[1.02479603 0.22320957]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.95  2.5 ]\n",
      "djdy:  [[1.00279603 0.21220957]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.75  1.2 ]\n",
      "djdy:  [[0.98179603 0.19520957]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5414, 0.4586]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5000, 0.4650]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.5096,  0.2604]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.95  0.  ]\n",
      "djdy:  [[1.04157626 0.22455297]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.8 -0.2]\n",
      "djdy:  [[1.03257626 0.19855297]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-0.7   0.65]\n",
      "djdy:  [[1.02057626 0.19755297]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-0.7 -0.8]\n",
      "djdy:  [[0.97657626 0.20955297]]\n",
      "FORWARD BEGIN\n",
      "input from forward:  tensor([[0.5372, 0.4628]], grad_fn=<SoftmaxBackward>)\n",
      "weight_matrix:  Parameter containing:\n",
      "tensor([[2.4631, 0.0576],\n",
      "        [0.9005, 1.9972]], dtype=torch.float64, requires_grad=True)\n",
      "wm:  [[2.463082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "result:  tensor([[0.5220, 0.4360]], dtype=torch.float64)\n",
      "FORWARD END\n",
      "BACKWARD BEGIN\n",
      "grad_output:  tensor([[-0.4959,  0.2530]], dtype=torch.float64)\n",
      "[[2.473082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.453082644583336, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [ 0.2  -0.15]\n",
      "djdy:  [[1.02890802 0.17797575]]\n",
      "[[2.4630826445833356, 0.06763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.047638811522027966], [0.9005272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [-1.35  0.3 ]\n",
      "djdy:  [[1.01890802 0.20097575]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9105272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.8905272584136453, 1.9971909533403034]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403034]]\n",
      "dydw:  [1.1  0.35]\n",
      "djdy:  [[0.99890802 0.20397575]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 2.007190953340303]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9871909533403032]]\n",
      "[[2.4630826445833356, 0.05763881152202797], [0.9005272584136453, 1.9971909533403032]]\n",
      "dydw:  [-1.   -0.35]\n",
      "djdy:  [[0.98990802 0.23197575]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-273cdd7d22db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Optimize the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mateusz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mateusz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trenowanie modelu\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = nn.NLLLoss()\n",
    "\n",
    "epochs = 20\n",
    "loss_list = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch: \", epoch)\n",
    "    print(\"weight_matrix: \", weight_matrix)\n",
    "#     print(\"wm: \", weight_matrix)\n",
    "    total_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         target = target.float() # for MSELoss() function\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        # Calculating loss\n",
    "        loss = loss_func(output, target)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss.append(loss.item())\n",
    "    loss_list.append(sum(total_loss)/len(total_loss))\n",
    "    print('Training [{:.0f}%]\\tLoss: {:.4f}'.format(\n",
    "        100. * (epoch + 1) / epochs, loss_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Neg Log Likelihood Loss')"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)\n",
    "plt.title('Hybrid NN Training Convergence')\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('Neg Log Likelihood Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mateusz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "c:\\users\\mateusz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test data:\n",
      "\tLoss: 0.0720\n",
      "\tAccuracy: 99.0%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model(data)\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        loss = loss_func(output, target)\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    print('Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%'.format(\n",
    "        sum(total_loss) / len(total_loss),\n",
    "        correct / len(test_loader) * 100)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mateusz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "c:\\users\\mateusz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples_show = 6\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if count == n_samples_show:\n",
    "            break\n",
    "        output = model(data)\n",
    "        \n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "\n",
    "        axes[count].imshow(data[0].numpy().squeeze(), cmap='gray')\n",
    "\n",
    "        axes[count].set_xticks([])\n",
    "        axes[count].set_yticks([])\n",
    "        axes[count].set_title('Predicted {}'.format(pred.item()))\n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nie trzeba odpalać STARE FUNKCJE\n",
    "epsilon = 0.01\n",
    "def update(ph, expected_ph, weight_matrix, lr):\n",
    "    gradient = []\n",
    "    for i, row in enumerate(weight_matrix):\n",
    "        gradient_row = []\n",
    "        for j, el in enumerate(row):\n",
    "            weight_matrix[i][j] += epsilon\n",
    "            result_plus = circuit_function(qc, weight_matrix)\n",
    "\n",
    "            weight_matrix[i][j] -= 2*epsilon\n",
    "            result_minus = circuit_function(qc, weight_matrix)\n",
    "\n",
    "            weight_matrix[i][j] += epsilon\n",
    "#             result_zero = circuit_function(qc, weight_matrix)\n",
    "#             print(\"ph\", result_zero)\n",
    "#             print(\"exp_ph\", expected_ph)\n",
    "#             print(\"el final:\", el)\n",
    "#             print(result_plus - result_minus)\n",
    "            result = (result_plus - result_minus)/(2*epsilon) * lr * (ph - expected_ph)\n",
    "            gradient_row.append(result)\n",
    "        gradient.append(gradient_row)\n",
    "#     print(\"gradient\", gradient)\n",
    "\n",
    "    weight_matrix = weight_matrix - gradient\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'x': array([1, 1]), 'y': 0}, {'x': array([0, 0]), 'y': 0}, {'x': array([0, 1]), 'y': 1}, {'x': array([1, 1]), 'y': 0}]\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(dataset_size):\n",
    "    dataset = []\n",
    "    for i in range(dataset_size):\n",
    "        x = np.array([random.choice([0,1]), random.choice([0,1])])\n",
    "        y = 1\n",
    "        if np.array_equal(x, np.array([0,0])) or np.array_equal(x, np.array([1,1])):\n",
    "            y = 0\n",
    "        dataset.append({\"x\": x, \"y\": y})\n",
    "    return dataset\n",
    "\n",
    "dataset = create_dataset(4)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-5b755048f991>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# x = np.array([random.uniform(0, 1) for n in range(visible)])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# template do uczenia\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#definicja rozmiaru sieci\n",
    "visible = 2\n",
    "hidden = 1\n",
    "ancilla = visible-1\n",
    "\n",
    "#definicja wejścia (x)oraz inicjalizacja macierzy wag\n",
    "# x = np.array([random.uniform(0, 1) for n in range(visible)])\n",
    "\n",
    "dataset = create_dataset(10)\n",
    "print(dataset[0][\"x\"][0])\n",
    "print(dataset[0][\"x\"][1])\n",
    "print([n for n in range(visible)])\n",
    "\n",
    "\n",
    "weight_matrix = np.random.rand(visible, hidden) * np.pi\n",
    "\n",
    "#definicja parametrów uczenia\n",
    "num_shots = 1000\n",
    "num_epochs = 100\n",
    "qr = QuantumRegister(visible + hidden + ancilla, 'q')\n",
    "cr = ClassicalRegister(hidden, 'c')\n",
    "qc = QuantumCircuit(qr, cr)\n",
    "\n",
    "cost_function_data = []\n",
    "lr = 0.05\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch: \", epoch)\n",
    "    for i, element in enumerate(dataset):\n",
    "#         print(element)\n",
    "        x = np.array([dataset[i][\"x\"][n] for n in range(visible)])\n",
    "        exp_ph = dataset[i][\"y\"]\n",
    "        ph = circuit_function(qc, weight_matrix)\n",
    "        weight_matrix = update(ph, exp_ph, weight_matrix, lr)\n",
    "#         print(\"exp_ph\", exp_ph, \"ph\", ph, \"weight_matrix\", weight_matrix, \"cost_function\", 0.5 * (ph - exp_ph)**2)   \n",
    "    cost_function_data.append(0.5 * (ph - exp_ph)**2)\n",
    "qc.draw()\n",
    "plt.xlabel('number of epochs')\n",
    "plt.ylabel('cost')\n",
    "plt.plot(cost_function_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n",
      "0 \n",
      "\n",
      "0.917\n",
      "1 \n",
      "\n",
      "0.045\n",
      "0 \n",
      "\n",
      "0.912\n",
      "1 \n",
      "\n",
      "0.909\n",
      "1 \n",
      "\n",
      "0.029\n",
      "0 \n",
      "\n",
      "0.053\n",
      "0 \n",
      "\n",
      "0.041\n",
      "0 \n",
      "\n",
      "0.034\n",
      "0 \n",
      "\n",
      "0.038\n",
      "0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset)):\n",
    "    x = np.array([dataset[i][\"x\"][n] for n in range(visible)])\n",
    "    exp_ph = dataset[i][\"y\"]\n",
    "    ph = circuit_function(qc, weight_matrix)\n",
    "    print(ph)\n",
    "    print(exp_ph, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌─────────────────┐┌───┐┌────────────┐        \n",
       "q_0: |0>┤ Initialize(0,1) ├┤ H ├┤ Ry(1.1361) ├──■─────\n",
       "        ├─────────────────┤├───┤├────────────┤  │     \n",
       "q_1: |0>┤ Initialize(0,1) ├┤ H ├┤ Ry(4.3279) ├──■─────\n",
       "        └─────────────────┘└───┘└────────────┘┌─┴─┐┌─┐\n",
       "q_2: |0>──────────────────────────────────────┤ X ├┤M├\n",
       "                                              └───┘└╥┘\n",
       "q_3: |0>────────────────────────────────────────────╫─\n",
       "                                                    ║ \n",
       " c_0: 0 ════════════════════════════════════════════╩═\n",
       "                                                      </pre>"
      ],
      "text/plain": [
       "<qiskit.visualization.text.TextDrawing at 0x7fbb1f4d74a8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qc.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.031\n"
     ]
    }
   ],
   "source": [
    "ph = circuit_function(qc, weight_matrix)\n",
    "print(ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
